---
layout: "@/layouts/block-docs-layout.astro"
title: ONNX_MODEL
---

{/* DO NOT EDIT THIS FILE! IT IS BEING AUTO GENERATED */}
{/* PLEASE REFER TO THE CONTRIBUTION GUIDE ON THE DOCS PAGE */}

import docstring from "@blocks/AI_ML/LOAD_MODEL/ONNX_MODEL/docstring.json";
import PythonDocsDisplay from "@/components/python-docs-display.astro";

<PythonDocsDisplay docstring={docstring} />

<details>
<summary>Python Code</summary>

```python
from flojoy import flojoy, run_in_venv, Vector
from flojoy.utils import FLOJOY_CACHE_DIR


@flojoy
@run_in_venv(
    pip_dependencies=[
        "onnxruntime",
        "numpy",
        "onnx",
    ]
)
def ONNX_MODEL(
    file_path: str,
    default: Vector,
) -> Vector:
    """ONNX_MODEL loads a serialized ONNX model and uses it to make predictions using ONNX Runtime.

    This allows supporting a wide range of deep learning frameworks and hardware platforms.

    Notes
    -----

    On the one hand, ONNX is an open format to represent deep learning models.
    ONNX defines a common set of operators - the building blocks of machine learning
    and deep learning models - and a common file format to enable AI developers
    to use models with a variety of frameworks, tools, runtimes, and compilers.

    See: https://onnx.ai/

    On the other hand, ONNX Runtime is a high-performance inference engine for machine
    learning models in the ONNX format. ONNX Runtime has proved to considerably increase
    performance in inferencing for a broad range of ML models and hardware platforms.

    See: https://onnxruntime.ai/docs/

    Moreover, the ONNX Model Zoo is a collection of pre-trained models for common
    machine learning tasks. The models are stored in ONNX format and are ready to use
    in different inference scenarios.

    See: https://github.com/onnx/models

    Parameters
    ----------
    file_path : str
        Path to a ONNX model to load and use for prediction.

    default : Vector
        The input tensor to use for prediction.
        For now, only a single input tensor is supported.
        Note that the input tensor shape is not checked against the model's input shape.

    Returns
    -------
    Vector:
        The predictions made by the ONNX model.
        For now, only a single output tensor is supported.
    """

    import os
    import onnx
    import urllib.request
    import numpy as np
    import onnxruntime as rt

    model_name = os.path.basename(file_path)

    if file_path.startswith("http://") or file_path.startswith("https://"):
        # Downloading the ONNX model from a URL to FLOJOY_CACHE_DIR.
        onnx_model_zoo_cache = os.path.join(
            FLOJOY_CACHE_DIR, "cache", "onnx", "model_zoo"
        )

        os.makedirs(onnx_model_zoo_cache, exist_ok=True)

        filename = os.path.join(onnx_model_zoo_cache, model_name)

        urllib.request.urlretrieve(
            url=file_path,
            filename=filename,
        )

        # Using the downloaded file.
        file_path = filename

    # Pre-loading the serialized model to validate whether is well-formed or not.
    model = onnx.load(file_path)
    onnx.checker.check_model(model)

    # Using ONNX runtime for the ONNX model to make predictions.
    sess = rt.InferenceSession(file_path, providers=["CPUExecutionProvider"])

    # TODO(jjerphan): Assuming a single input and a single output for now.
    input_name = sess.get_inputs()[0].name
    label_name = sess.get_outputs()[0].name

    # TODO(jjerphan): For now NumPy is assumed to be the main backend for Flojoy.
    # We might adapt it in the future so that we can use other backends
    # for tensor libraries for application using Deep Learning libraries.
    input_tensor = np.asarray(default.v, dtype=np.float32)
    predictions = sess.run([label_name], {input_name: input_tensor})[0]

    return Vector(v=predictions)

```

[Find this Flojoy Block on GitHub](https://github.com/flojoy-ai/blocks/blob/main/blocks/AI_ML/LOAD_MODEL/ONNX_MODEL/ONNX_MODEL.py)

</details>

## Example

import GetHelpWidget from "@/components/get-help-widget.astro";

<GetHelpWidget />

import app from "@blocks/AI_ML/LOAD_MODEL/ONNX_MODEL/app.json";
import AppDisplay from "@/components/app-display.tsx";

<AppDisplay app={app} blockName="ONNX_MODEL" client:visible />

import Example from "@blocks/AI_ML/LOAD_MODEL/ONNX_MODEL/example.md";

<Example />

{/* DO NOT EDIT THIS FILE! IT IS BEING AUTO GENERATED */}
{/* PLEASE REFER TO THE CONTRIBUTION GUIDE ON THE DOCS PAGE */}
